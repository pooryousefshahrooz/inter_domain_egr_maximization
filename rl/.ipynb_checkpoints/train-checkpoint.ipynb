{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc99fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from game import CFRRL_Game\n",
    "from model import Model\n",
    "from network import Network\n",
    "from config import get_config\n",
    "from solver import Solver\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('num_agents',1, 'number of agents')\n",
    "flags.DEFINE_string('baseline', 'avg', 'avg: use average reward as baseline, best: best reward as baseline')\n",
    "flags.DEFINE_integer('num_iter', 20, 'Number of iterations each agent would run')\n",
    "# print(FLAGS.num_agents)\n",
    "import pdb\n",
    "# pdb.set_trace()\n",
    "GRADIENTS_CHECK=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560282a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5866ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21efc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_agent(config, game, model_weights_queues, experience_queues):\n",
    "    model = Model(config, game.state_dims, game.action_dim, game.max_moves, master=True)\n",
    "    model.save_hyperparams(config)\n",
    "    start_step = model.restore_ckpt()\n",
    "    for step in tqdm(range(start_step, config.max_step), ncols=70, initial=start_step):\n",
    "        model.ckpt.step.assign_add(1)\n",
    "        model_weights = model.model.get_weights()\n",
    "\n",
    "        for i in range(FLAGS.num_agents):\n",
    "            model_weights_queues[i].put(model_weights)\n",
    "\n",
    "        if config.method == 'actor_critic':\n",
    "            #assemble experiences from the agents\n",
    "            s_batch = []\n",
    "            a_batch = []\n",
    "            r_batch = []\n",
    "\n",
    "            for i in range(FLAGS.num_agents):\n",
    "                s_batch_agent, a_batch_agent, r_batch_agent = experience_queues[i].get()\n",
    "              \n",
    "                assert len(s_batch_agent) == FLAGS.num_iter, \\\n",
    "                    (len(s_batch_agent), len(a_batch_agent), len(r_batch_agent))\n",
    "\n",
    "                s_batch += s_batch_agent\n",
    "                a_batch += a_batch_agent\n",
    "                r_batch += r_batch_agent\n",
    "           \n",
    "            assert len(s_batch)*game.max_moves == len(a_batch)\n",
    "            #used shared RMSProp, i.e., shared g\n",
    "            actions = np.eye(game.action_dim, dtype=np.float32)[np.array(a_batch)]\n",
    "            value_loss, entropy, actor_gradients, critic_gradients = model.actor_critic_train(np.array(s_batch), \n",
    "                                                                    actions, \n",
    "                                                                    np.array(r_batch).astype(np.float32), \n",
    "                                                                    config.entropy_weight)\n",
    "       \n",
    "            if GRADIENTS_CHECK:\n",
    "                for g in range(len(actor_gradients)):\n",
    "                    assert np.any(np.isnan(actor_gradients[g])) == False, ('actor_gradients', s_batch, a_batch, r_batch, entropy)\n",
    "                for g in range(len(critic_gradients)):\n",
    "                    assert np.any(np.isnan(critic_gradients[g])) == False, ('critic_gradients', s_batch, a_batch, r_batch)\n",
    "\n",
    "            if step % config.save_step == config.save_step - 1:\n",
    "                model.save_ckpt(_print=True)\n",
    "                \n",
    "                #log training information\n",
    "                actor_learning_rate = model.lr_schedule(model.actor_optimizer.iterations.numpy()).numpy()\n",
    "                avg_value_loss = np.mean(value_loss)\n",
    "                avg_reward = np.mean(r_batch)\n",
    "                avg_entropy = np.mean(entropy)\n",
    "            \n",
    "                model.inject_summaries({\n",
    "                    'learning rate': actor_learning_rate,\n",
    "                    'value loss': avg_value_loss,\n",
    "                    'avg reward': avg_reward,\n",
    "                    'avg entropy': avg_entropy\n",
    "                    }, step)\n",
    "                print('lr:%f, value loss:%f, avg reward:%f, avg entropy:%f'%(actor_learning_rate, avg_value_loss, avg_reward, avg_entropy))\n",
    "\n",
    "        elif config.method == 'pure_policy':\n",
    "            #assemble experiences from the agents\n",
    "            s_batch = []\n",
    "            a_batch = []\n",
    "            r_batch = []\n",
    "            ad_batch = []\n",
    "\n",
    "            for i in range(FLAGS.num_agents):\n",
    "                s_batch_agent, a_batch_agent, r_batch_agent, ad_batch_agent = experience_queues[i].get()\n",
    "              \n",
    "                assert len(s_batch_agent) == FLAGS.num_iter, \\\n",
    "                    (len(s_batch_agent), len(a_batch_agent), len(r_batch_agent), len(ad_batch_agent))\n",
    "\n",
    "                s_batch += s_batch_agent\n",
    "                a_batch += a_batch_agent\n",
    "                r_batch += r_batch_agent\n",
    "                ad_batch += ad_batch_agent\n",
    "           \n",
    "            assert len(s_batch)*game.max_moves == len(a_batch)\n",
    "            #used shared RMSProp, i.e., shared g\n",
    "            actions = np.eye(game.action_dim, dtype=np.float32)[np.array(a_batch)]\n",
    "            entropy, gradients = model.policy_train(np.array(s_batch), \n",
    "                                                      actions, \n",
    "                                                      np.vstack(ad_batch).astype(np.float32), \n",
    "                                                      config.entropy_weight)\n",
    "\n",
    "            if GRADIENTS_CHECK:\n",
    "                for g in range(len(gradients)):\n",
    "                    assert np.any(np.isnan(gradients[g])) == False, (s_batch, a_batch, r_batch)\n",
    "            \n",
    "            if step % config.save_step == config.save_step - 1:\n",
    "                if step <100:\n",
    "                    time.sleep(60*60)\n",
    "                model.save_ckpt(_print=True)\n",
    "                \n",
    "                #log training information\n",
    "                learning_rate = model.lr_schedule(model.optimizer.iterations.numpy()).numpy()\n",
    "                avg_reward = np.mean(r_batch)\n",
    "                avg_advantage = np.mean(ad_batch)\n",
    "                avg_entropy = np.mean(entropy)\n",
    "                model.inject_summaries({\n",
    "                    'learning rate': learning_rate,\n",
    "                    'avg reward': avg_reward,\n",
    "                    'avg advantage': avg_advantage,\n",
    "                    'avg entropy': avg_entropy\n",
    "                    }, step)\n",
    "                print('lr:%f, avg reward:%f, avg advantage:%f, avg entropy:%f'%(learning_rate, avg_reward, avg_advantage, avg_entropy))\n",
    "\n",
    "def agent(agent_id, config, game,network, tm_subset, model_weights_queue, experience_queue):\n",
    "    random_state = np.random.RandomState(seed=agent_id)\n",
    "    model = Model(config, game.state_dims, game.action_dim, game.max_moves, master=False)\n",
    "    solver = Solver()\n",
    "    # initial synchronization of the model weights from the coordinator \n",
    "    model_weights = model_weights_queue.get()\n",
    "    model.model.set_weights(model_weights)\n",
    "\n",
    "    idx = 0\n",
    "    s_batch = []\n",
    "    a_batch = []\n",
    "    r_batch = []\n",
    "    if config.method == 'pure_policy':\n",
    "        ad_batch = []\n",
    "    run_iteration_idx = 0\n",
    "    num_tms = len(tm_subset)\n",
    "    random_state.shuffle(tm_subset)\n",
    "    run_iterations = FLAGS.num_iter\n",
    "    \n",
    "    while True:\n",
    "        tm_idx = tm_subset[idx]\n",
    "        #state\n",
    "        state = game.get_state(tm_idx)\n",
    "        #print(\"got the state\")\n",
    "        s_batch.append(state)\n",
    "        #action\n",
    "        if config.method == 'actor_critic':    \n",
    "            policy = model.actor_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "        elif config.method == 'pure_policy':\n",
    "            policy = model.policy_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "        #print(\"np.count_nonzero(policy) >= game.max_moves, (policy, state)\",np.count_nonzero(policy) , game.max_moves)\n",
    "        #print(\"(policy, state)\",(policy, state))\n",
    "        assert np.count_nonzero(policy) >= game.max_moves, (policy, state)\n",
    "        actions = random_state.choice(game.action_dim, game.max_moves, p=policy, replace=False)\n",
    "        for a in actions:\n",
    "            a_batch.append(a)\n",
    "\n",
    "        #reward\n",
    "        reward = game.reward(tm_idx,network,actions,solver)\n",
    "        #print(\"reward is \",reward)\n",
    "        r_batch.append(reward)\n",
    "       \n",
    "        if config.method == 'pure_policy':\n",
    "            #advantage\n",
    "            if config.baseline == 'avg':\n",
    "                ad_batch.append(game.advantage(tm_idx, reward))\n",
    "                game.update_baseline(tm_idx, reward)\n",
    "            elif config.baseline == 'best':\n",
    "                best_actions = policy.argsort()[-game.max_moves:]\n",
    "                best_reward = game.reward(tm_idx, best_actions)\n",
    "                ad_batch.append(reward - best_reward)\n",
    "\n",
    "        run_iteration_idx += 1\n",
    "        if run_iteration_idx >= run_iterations:\n",
    "            # Report experience to the coordinator                          \n",
    "            if config.method == 'actor_critic':    \n",
    "                experience_queue.put([s_batch, a_batch, r_batch])\n",
    "            elif config.method == 'pure_policy':\n",
    "                experience_queue.put([s_batch, a_batch, r_batch, ad_batch])\n",
    "            \n",
    "            #print('report', agent_id)\n",
    "\n",
    "            # synchronize the network parameters from the coordinator\n",
    "            model_weights = model_weights_queue.get()\n",
    "            model.model.set_weights(model_weights)\n",
    "            \n",
    "            del s_batch[:]\n",
    "            del a_batch[:]\n",
    "            del r_batch[:]\n",
    "            if config.method == 'pure_policy':\n",
    "                del ad_batch[:]\n",
    "            run_iteration_idx = 0\n",
    "      \n",
    "        # Update idx\n",
    "        idx += 1\n",
    "        if idx == num_tms:\n",
    "           random_state.shuffle(tm_subset)\n",
    "           idx = 0\n",
    "\n",
    "def main(_):\n",
    "    #cpu only\n",
    "    tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "    #tf.get_logger().setLevel('INFO')\n",
    "    #tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "    config = get_config(FLAGS) or FLAGS\n",
    "    \n",
    "    \n",
    "    for num_paths in range(int(config.min_num_of_paths),int(config.num_of_paths)+1):\n",
    "        for network_topology in config.topology_file:\n",
    "            for edge_capacity_bound in config.edge_capacity_bounds:\n",
    "                network = Network(config,edge_capacity_bound,False)\n",
    "                for fidelity_threshold_up_range in config.fidelity_threshold_ranges:\n",
    "                    network.fidelity_threshold_range = fidelity_threshold_up_range\n",
    "                    network.set_each_wk_k_fidelity_threshold()\n",
    "                    for edge_fidelity_range in config.edge_fidelity_ranges:\n",
    "                        network.end_level_purification_flag = True\n",
    "                        network.set_edge_fidelity(edge_fidelity_range)\n",
    "                        # we get all the paths for all workloads\n",
    "                        network.num_of_paths = num_paths\n",
    "                        network.get_path_info()\n",
    "                        \n",
    "                        \"\"\"we remove all the checkpoints for not using trained model of previous #paths setup\"\"\"\n",
    "                        try:\n",
    "                            shutil.rmtree(config.tf_ckpts)\n",
    "                        except:\n",
    "                            pass\n",
    "                        \"\"\"we first find the candidate paths and use it for action dimention\"\"\"\n",
    "                        # we se the state dimention and action dimention\n",
    "                        game = CFRRL_Game(config,network)\n",
    "    \n",
    "                        \n",
    "                        model_weights_queues = []\n",
    "                        experience_queues = []\n",
    "                        if FLAGS.num_agents == 0 or FLAGS.num_agents >= mp.cpu_count():\n",
    "                            FLAGS.num_agents = mp.cpu_count() - 1\n",
    "                        print('Agent num: %d, iter num: %d\\n'%(FLAGS.num_agents+1, FLAGS.num_iter))\n",
    "                        for _ in range(FLAGS.num_agents):\n",
    "                            model_weights_queues.append(mp.Queue(1))\n",
    "                            experience_queues.append(mp.Queue(1))\n",
    "\n",
    "                        tm_subsets = np.array_split(game.wk_indexes, FLAGS.num_agents)\n",
    "\n",
    "                        coordinator = mp.Process(target=central_agent, args=(config, game, model_weights_queues, experience_queues))\n",
    "\n",
    "                        coordinator.start()\n",
    "\n",
    "                        agents = []\n",
    "                        for i in range(FLAGS.num_agents):\n",
    "                            agents.append(mp.Process(target=agent, args=(i, config, game, network,tm_subsets[i], model_weights_queues[i], experience_queues[i])))\n",
    "\n",
    "                        for i in range(FLAGS.num_agents):\n",
    "                            agents[i].start()\n",
    "\n",
    "                        coordinator.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c8992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51679d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4841ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# tm_indexes = np.arange(0, 100)\n",
    "# print(tm_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e37aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
